{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torchvision.models import resnet50\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "class FaceNetModel(nn.Module):\n",
    "    def __init__(self, pretrained=False):\n",
    "        super(FaceNetModel, self).__init__()\n",
    "\n",
    "        self.model = resnet50(pretrained)\n",
    "        embedding_size = 128\n",
    "        num_classes = 500\n",
    "        self.cnn = nn.Sequential(\n",
    "            self.model.conv1,\n",
    "            self.model.bn1,\n",
    "            self.model.relu,\n",
    "            self.model.maxpool,\n",
    "            self.model.layer1,\n",
    "            self.model.layer2,\n",
    "            self.model.layer3,\n",
    "            self.model.layer4)\n",
    "\n",
    "        # modify fc layer based on https://arxiv.org/abs/1703.07737\n",
    "        self.model.fc = nn.Sequential(\n",
    "            Flatten(),\n",
    "            # nn.Linear(100352, 1024),\n",
    "            # nn.BatchNorm1d(1024),\n",
    "            # nn.ReLU(),\n",
    "            nn.Linear(100352, embedding_size))\n",
    "\n",
    "        self.model.classifier = nn.Linear(embedding_size, num_classes)\n",
    "\n",
    "    def l2_norm(self, input):\n",
    "        input_size = input.size()\n",
    "        buffer = torch.pow(input, 2)\n",
    "        normp = torch.sum(buffer, 1).add_(1e-10)\n",
    "        norm = torch.sqrt(normp)\n",
    "        _output = torch.div(input, norm.view(-1, 1).expand_as(input))\n",
    "        output = _output.view(input_size)\n",
    "        return output\n",
    "\n",
    "    def freeze_all(self):\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_all(self):\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def freeze_fc(self):\n",
    "        for param in self.model.fc.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_fc(self):\n",
    "        for param in self.model.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def freeze_only(self, freeze):\n",
    "        for name, child in self.model.named_children():\n",
    "            if name in freeze:\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = False\n",
    "            else:\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "    def unfreeze_only(self, unfreeze):\n",
    "        for name, child in self.model.named_children():\n",
    "            if name in unfreeze:\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = True\n",
    "            else:\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "    # returns face embedding(embedding_size)\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = self.model.fc(x)\n",
    "\n",
    "        features = self.l2_norm(x)\n",
    "        # Multiply by alpha = 10 as suggested in https://arxiv.org/pdf/1703.09507.pdf\n",
    "        alpha = 10\n",
    "        features = features * alpha\n",
    "        return features\n",
    "\n",
    "    def forward_classifier(self, x):\n",
    "        features = self.forward(x)\n",
    "        res = self.model.classifier(features)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FaceNetModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path='../code/log/best_state_2.pth'\n",
    "device = torch.device(\"cuda:0\")\n",
    "model = FaceNetModel()\n",
    "model.to(device)\n",
    "model.eval()\n",
    "state = torch.load(path, map_location=lambda storage, loc: storage)\n",
    "model.load_state_dict(state[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "trfrm = transforms.Compose([\n",
    "    lambda x: x.convert('RGB'),\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])])\n",
    "topil = transforms.ToPILImage()\n",
    "totensor = transforms.Compose(trfrm.transforms[:-1])\n",
    "\n",
    "def get_distance(img1, img2,model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x1 = trfrm(img1).unsqueeze(0)\n",
    "        x2 = trfrm(img2).unsqueeze(0)\n",
    "        x1,x2 = x1.to('cuda:0'), x2.to('cuda:0')\n",
    "        embed1 = model(x1)\n",
    "        embed2 = model(x2)\n",
    "        return F.pairwise_distance(embed1, embed2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = 'C:/Users/Gemmechu/Documents/files/UMich/DogOwner/code/datasets/dogOwner/7/1.jpg'\n",
    "img2 = 'C:/Users/Gemmechu/Documents/files/UMich/DogOwner/code/datasets/dogOwner/7/2.jpg'\n",
    "img3 = 'C:/Users/Gemmechu/Pictures/dog_and_owner/Newfolder/sarah.png'\n",
    "img4 = 'C:/Users/Gemmechu/Pictures/dog_and_owner/Newfolder/sarahd.jpg'\n",
    "imgA = Image.open(img3).convert('RGB')\n",
    "imgB = Image.open(img4).convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9936], device='cuda:0')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist = get_distance(imgA, imgB,model)\n",
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
